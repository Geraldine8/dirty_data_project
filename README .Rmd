---
title: "Dirty Data Project"
author: "Geraldine A."
output:
  prettydoc::html_pretty:
    theme: hpstr
    highlight: github

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load-packages, include=FALSE}

library(knitr)
library(readr)
library(here)
library(tidyverse)
library(kableExtra)

```

# Introduction

<div style="text-align: justify">
The dataset used in this analysis was collected from surveys conducted between 2015 and 2017, which gathered responses from individuals across various countries including the UK, USA, Spain, the Netherlands, and other European and Asian countries. The surveys asked participants to rate their level of satisfaction (on a scale of Joy, Despair, or Meh) when receiving different types of candies and treats during Halloween.

The purpose of this analysis is to identify patterns and insights within the dataset to better understand the preferences and satisfaction levels of individuals when it comes to Halloween candies and treats. By analyzing the data, we hope to gain a deeper understanding of the factors that influence people's satisfaction levels and how these factors differ across various countries and regions.

In the following sections, we will explore the dataset and present our findings in a clear and concise manner.

</div>

## Data 

<div style="text-align: justify">
For this analysis, I worked with three different data files that needed to be standardized, cleaned, and merged into a single file. The cleaning process involved removing any missing or incorrect data and handling inconsistencies in the formatting of the data.

One of the biggest challenges I encountered during the cleaning process was the lack of validation in the "country" column of the survey data. Respondents were allowed to enter any value they wanted in this column, leading to a large number of invalid or nonsensical responses. To address this issue, I implemented a process to identify and correct any erroneous data in the "country" column.

Despite these challenges, I was able to successfully prepare the data for analysis and extract valuable insights from it. The next sections will describe the analysis I performed on the data and the results I obtained.
</div>

## Analysis and visualisations



```{r echo=FALSE, include="FALSE"}
clean_data <- read_csv(here("data-clean/clean_data.csv"))

```

#### Total number of candy ratings given across the three years: 


```{r, echo=FALSE}
# Define column names to exclude from the count
cols_to_exclude <- c("how_old_are_you?", "trick_or_treat", "gender",
                    "country")

# Number of non-na ratings
num_of_non_rating <- clean_data %>%
  select(-one_of(cols_to_exclude)) %>%
  summarise_all(~ sum(!is.na(.))) %>%
  summarise(total_sum_ratings = sum(.))

kable(num_of_non_rating, format = "html", align = 'l') %>%
  kable_styling(full_width = T, position = "left")



```



#### Average age of people who are going out trick or treating?

```{r, echo=FALSE}

avg_age_that_who_going_out <- clean_data %>% 
  select(`how_old_are_you?`, trick_or_treat) %>% 
  filter(trick_or_treat == "yes") %>% 
  summarise(avg_age = round(mean(`how_old_are_you?`, na.rm = TRUE),0))

kable(avg_age_that_who_going_out, format = "html", align = 'l') %>%
  kable_styling(full_width = T, position = "left")
```


#### Top three the most popular candy bar by year 

```{r missing_data_by_year, echo=FALSE}

missing_data_by_year <- clean_data %>%
  group_by(year) %>% 
  summarise(across(everything(), ~sum(is.na(.)))) %>% 
  mutate(total = rowSums(select(., -year))) %>% 
  select(year, total) %>% 
  ggplot(aes(x = year, y = total, fill = factor(year))) +
  geom_bar(stat = "identity") +
  labs(title = "Missing Data by Year", x = "Year", y = "Total Missing Data", fill = "Year") +
  scale_fill_brewer(palette = "Dark2")+
  scale_y_continuous(labels = scales::comma_format(scale = 1)) 

plot(missing_data_by_year)
```






Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
